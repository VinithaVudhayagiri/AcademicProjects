---
title: "EAS509_Project2"
author: "Vinitha Vudhayagiri"
date: "2023-05-05"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##### Libraries used are

```{r}
library(forecast)
library(tseries)
library(zoo)
library(knitr)
```

#### **1. Reading the CSV data file**

```{r}
oil_data <- read.csv("/Users/vinithavudhayagiri/Documents/Spring2023/SDM2/Project2/oil.csv")
# Check the dimensions of the data frame
dim(oil_data)
# printing the first few rows of the data
head(oil_data)
```

#### **2.Plotting the Time Series for the accessed data**

```{r}
# formatting the date column 
oil_data$date <- as.Date(oil_data$date)
```

```{r}
plot(oil_data,ylab='Price',xlab='Date',type='l')
title("Oil Prices ")
```

From the above plot we can find that there is some discontinuity in the graph, as there are some missing values in the dataset there is cut at that slot in the graph. So, we have to replace the Null values for getting continuous plot for further analysis.


#### **3.Finding the missing values and imputing the data.**

```{r}
# checking if there are any null values
sum(is.na(oil_data))
# filling the data with the previous observation
oil_data$dcoilwtico = na.locf(oil_data$dcoilwtico, fromLast = TRUE)
# checking for the null values again to confirm if the data is being filled
sum(is.na(oil_data))
```

There are 43 missing values in the dataset. We can use one of five methods to impute the data: forward fill, backward fill, mean imputation, linear interpolation, or seasonal interpolation. We selected backward fill because the prices are almost nearly equal for 5 to 7 days and the mean of the total series is far different from the values in the surrounding of the missing point.

#### **4.Plotting the time series for the updated data(without any missing data)**

```{r}
plot(oil_data,ylab='Price',xlab='Date',type='l')
title("Updated Oil Prices ")
```

The above plot is the time series plot after filling the missing values. Now we can see that there is no discontinuity in the graph.The graph shows that there was a sharp decrease in sales in the middle of 2014. Sales remained low after that, and there was no further decrease up to 2018. However, if we look at the monthly data, we can see that there is a trend of sales increasing in the beginning of the year and then decreasing in the middle of the year. Sales remain within the same range with little fluctuations after the middle of the year. The graph does not show any seasonality.

#### **5. Performing ETS Models and Holt-Winters Models**

```{r}
# Creating the time series object for the oil_data
oil_ts <- ts(oil_data$dcoilwtico, start = c(1986, 1), frequency = 12)
# Printing the time series object we created
oil_ts
```

```{r}
# Perform the ADF test for stationarity
adf_test <- adf.test(oil_ts)

# Print the results
cat("ADF Statistic:", adf_test$statistic, "\n")
cat("p-value:", adf_test$p.value, "\n")
cat("Critical values:", adf_test$critical, "\n")
```

The data is not stationary. We can tell whether the process is stationary or not by looking at the p-value from the Augmented Dickey-Fuller (ADF) test. If the p-value is less than 0.5, then the process is stationary. If the p-value is greater than 0.5, then the process is non-stationary. In this case, the p-value from the ADF test is 0.81, which is greater than 0.5. Therefore, the process is non-stationary.

```{r}
# Decomposing the time series object to different components
components_oil_data <- decompose(oil_ts)
# Plotting the decomposed time series object
plot(components_oil_data)
```

#### **ETS Models**

ETS models are a family of time series forecasting methods that decompose a time series into three components: level, trend, and seasonality. It stands for Exponential Smoothing Time Series Analysis. It is a forecasting technique that uses weighted averages of past data to predict future values. It is a popular forecasting method because it is relatively easy to understand and implement.

ETS models are typically used to forecast time series that do not have a lot of noise. They are also a good choice for forecasting time series that have trend and seasonality components.

There are three main types of ETS models:

Additive ETS (AETS): This model assumes that the trend and seasonality components are additive, meaning that they can be added together to get the overall time series.
Multiplicative ETS (METS): This model assumes that the trend and seasonality components are multiplicative, meaning that they are multiplied together to get the overall time series.
Damped ETS (DETS): This model assumes that the trend component is damped, meaning that it will eventually level off.
ETS models are relatively easy to understand and implement. They can be used to forecast a variety of time series, including those with trend, seasonality, or both. They are relatively accurate, especially for short-term forecasts. However, they can be sensitive to outliers. They can also be computationally expensive for large datasets. They may not be as accurate for long-term forecasts.

Overall, ETS models are a powerful tool for time series forecasting. They are relatively easy to understand and implement. They can be used to forecast a variety of time series, including those with trend, seasonality, or both.

#### **1.ETS ANN Model**

```{r}
# Split the data into train and test sets
train <- oil_ts[1:1000]
test <- oil_ts[1001:1218]

# Fit a ETS model to the training data
ets_model1 <- ets(train)

# Make a forecast for the test data
forecast <- forecast(ets_model1, h = length(test))

# Calculate accuracy measures
accuracy(forecast, test)
```

```{r}
summary(ets_model1)
```

```{r}
# Make a forecast for the next 12 time periods
forecast <- forecast(ets_model1, h = 12)

# Plot the forecast and the actual data
plot(forecast)
lines(oil_ts, col = "blue")
```

```{r}
checkresiduals(ets_model1)
```

#### **2.ETS MNN Model**

```{r}
# Fit a Holt-Winters model to the training data
ets_model2 <- ets(train, model = "MNN")

# Make a forecast for the test data
forecast <- forecast(ets_model2, h = length(test))

# Calculate accuracy measures
accuracy(forecast, test)
```

```{r}
summary(ets_model2)
```

```{r}
# Make a forecast for the next 12 time periods
forecast <- forecast(ets_model2, h = 12)

# Plot the forecast and the actual data
plot(forecast)
lines(oil_ts, col = "blue")
```

```{r}
checkresiduals(ets_model2)
```

#### **3.ETS MMN Model**

```{r}
# Fit a Holt-Winters model to the training data
ets_model3 <- ets(train, model = "MMN")

# Make a forecast for the test data
forecast <- forecast(ets_model3, h = length(test))

# Calculate accuracy measures
accuracy(forecast, test)
```

```{r}
summary(ets_model3)
```

```{r}
# Make a forecast for the next 12 time periods
forecast <- forecast(ets_model3, h = 12)

# Plot the forecast and the actual data
plot(forecast)
lines(oil_ts, col = "blue")
checkresiduals(ets_model3)
```


#### **Holt- Winters Model**

Holt-Winters method, also known as triple exponential smoothing, is a forecasting method that takes into account the trend, seasonality, and level (i.e., baseline) of a time series. It uses three smoothing equations to generate forecasts: one for the level, one for the trend, and one for the seasonality.

The method is suitable for time series that have trend and/or seasonality, and the forecasts generated by the model tend to be more accurate than those generated by simple exponential smoothing or ARIMA models in cases where the data exhibit both trend and seasonality.

Advantages:

Easy to understand and implement
Can be used to forecast a variety of time series data, including those with trend, seasonality, or both
Relatively accurate, especially for short-term forecasts

Disadvantages:

Can be sensitive to outliers
Can be computationally expensive for large datasets
May not be as accurate for long-term forecasts

#### **1.Holt-Winters additive Model**

```{r}
# Convert the data to a time series object
oil_ts <- ts(oil_data$dcoilwtico, frequency = 5)

# Fit a model to the training data
hw_model1 <- hw(oil_ts)
summary(hw_model1)
```

```{r}
# Make a forecast for the next 12 time periods
forecast <- forecast(hw_model1, h = 5)

# Plot the forecast and the actual data
plot(forecast)
lines(oil_ts, col = "blue")
```

#### **2.Holt-Winters Multiplicative Model**

```{r}
# Fit a model to the training data
hw_model1 <- hw(oil_ts,seasonal="multiplicative" )
summary(hw_model1)
```

```{r}
# Make a forecast for the next 12 time periods
forecast <- forecast(hw_model1, h = 5)

# Plot the forecast and the actual data
plot(forecast)
lines(oil_ts, col = "blue")
```
#### **6 & 7 Suitable Models for the Time Series and Checking the adequacy**

#### **ARIMA Model**

ARIMA is a statistical model that is used to forecast time series data. It is relatively easy to understand and implement, and it can be used to forecast a variety of time series data, including those with trend, seasonality, or both.

Advantages:

Easy to understand and implement
Can be used to forecast a variety of time series data
Relatively accurate, especially for short-term forecasts

Disadvantages:

Can be sensitive to outliers
Can be computationally expensive for large datasets
May not be as accurate for long-term forecasts

```{r}
# Convert the data to time series object
oil_ts <- ts(oil_data$dcoilwtico, frequency = 365)
# Fit an ARIMA model to the time series
arima_model <- auto.arima(oil_ts, seasonal = TRUE)
# Print the model summary
summary(arima_model)
arima_forecast <- forecast(arima_model, h = 12)
plot(arima_forecast)
lines(oil_ts, col = "blue")
```

#### **SARIMA Model**

SARIMA stands for Seasonal Autoregressive Integrated Moving Average.
It is a statistical model that is used to forecast time series data that has seasonality.
It is a generalization of ARIMA models.
It is relatively easy to understand and implement.
It can be used to forecast a variety of time series data, including those with trend, seasonality, or both.
It is relatively accurate, especially for short-term forecasts.
However, it can be sensitive to outliers, computationally expensive for large datasets, and may not be as accurate for long-term forecasts.



```{r}
# Fit a SARIMA model to the data
sarima_model <- arima(oil_ts, order = c(1, 1, 1), seasonal = list(order = c(1, 0, 0), period = 7))
summary(sarima_model)
# Make a forecast for the next 12 time periods
sarima_forecast <- forecast(sarima_model, h = 12)
# Plot the forecast and the actual data
plot(sarima_forecast)
lines(oil_ts, col = "blue")
```

#### **8 Comparision between the models**

```{r}
# Create a data frame
df <- data.frame(
Model = c("ETS(A,N,N)", "ETS(M,N,N)", "ETS(M,M,N)", "Holt-Winters(Additive)", "Holt-Winters(Multiplicative)", "ARIMA", "SARIMA"),
RMSE = c(1.263,1.263,1.262,1.203, 1.226,1.203,1.199),

AIC = c(7381.89,7772.95,7776.70,9124.79,9544.23,3906.73,3906.46),

BIC = c(7396.61,7787.67,7801.24,9175.84,9595.28,3911.83,"-")
)
# Create a table
kable(df, caption = "Comparision Table of Model Performance")
```


AIC (Akaike Information Criterion) , BIC (Bayesian Information Criterion), RMSE(Root Mean Squared Error) are statistical measures used for model selection in time series analysis. These measures evaluate the tradeoff between the goodness of fit of a model and the complexity of the model.

The AIC value is calculated as 2k - 2ln(L), where k is the number of parameters in the model and ln(L) is the natural logarithm of the likelihood function of the model. The AIC value penalizes models for having more parameters, so a lower AIC value indicates a better model fit.

The BIC value is similar to the AIC, but it includes a larger penalty for models with more parameters. The BIC value is calculated as kln(n) - 2ln(L), where k is the number of parameters in the model, n is the sample size, and ln(L) is the natural logarithm of the likelihood function of the model.

Root Mean Square Error (RMSE) is a measure of the accuracy of forecasts. It is calculated by taking the square root of the average squared error between the actual values and the forecast values. RMSE is a useful measure of accuracy because it is not affected by the scale of the data. It is calculated by RMSE = sqrt(mean( (actual - forecast)^2 ))

In general, a lower AIC or BIC value indicates a better model fit. Therefore, when comparing different models, we choose the model with the lowest AIC or BIC value. However, AIC and BIC should not be used as absolute measures of model fit, and they should be used in combination with other diagnostic tests to ensure that the selected model adequately captures the patterns and dynamics of the time series.

The table shows that the SARIMA model has the lowest RMSE and AIC values. However, the BIC value for the SARIMA model is not available because the values did not converge to a solution. Since the difference between the RMSE and AIC values for the SARIMA and ARIMA models is small, we can use the ARIMA model for the analysis.


Team Members:
Alekhya Monavarthi -50469035 
Jayasree Keesari - 50477962 
Preethi Abhilasha Vaddi-50483865 
Vinitha Vudhayagiri-50478854
